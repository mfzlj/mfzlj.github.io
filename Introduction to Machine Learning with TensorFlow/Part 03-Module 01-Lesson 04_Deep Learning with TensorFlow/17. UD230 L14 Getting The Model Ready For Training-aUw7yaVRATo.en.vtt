WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.485
In the previous notebook,

00:00:01.485 --> 00:00:04.064
you learned how to build neural networks.

00:00:04.064 --> 00:00:06.840
In this notebook, we're going to learn how to train them.

00:00:06.840 --> 00:00:11.340
What we mean by training is that we want to feed an image to our network,

00:00:11.340 --> 00:00:13.965
such as the image of this four here,

00:00:13.965 --> 00:00:19.500
and we want our network to output a probability distribution such as this one,

00:00:19.500 --> 00:00:24.315
where there is a large probability of the digit in the image of being a four.

00:00:24.315 --> 00:00:26.820
As you learned in previous lessons,

00:00:26.820 --> 00:00:28.875
in order to train a neural network,

00:00:28.875 --> 00:00:30.975
you need a loss function.

00:00:30.975 --> 00:00:35.160
This loss function is also sometimes called the cost function.

00:00:35.160 --> 00:00:38.855
The goal of training a neural network is to adjust

00:00:38.855 --> 00:00:42.690
its parameters so as to minimize the value of our loss function,

00:00:42.690 --> 00:00:48.265
and we do this by using gradient descent and back-propagation.

00:00:48.265 --> 00:00:51.965
So let's see how we can train our models using TensorFlow.

00:00:51.965 --> 00:00:56.240
As always, we begin by importing the packages we're going to need.

00:00:56.240 --> 00:01:00.350
These are the same packages and inputs we did in the previous notebook,

00:01:00.350 --> 00:01:01.970
so there's nothing new here.

00:01:01.970 --> 00:01:04.735
Next, we load our dataset.

00:01:04.735 --> 00:01:09.050
In this notebook, we're also going to use the MNIST dataset.

00:01:09.050 --> 00:01:11.225
So we're going to load it by using

00:01:11.225 --> 00:01:14.515
TensorFlow datasets as we did in the previous notebook.

00:01:14.515 --> 00:01:17.340
The next step is to create our pipeline.

00:01:17.340 --> 00:01:20.705
This is the same pipeline we used in the previous notebook,

00:01:20.705 --> 00:01:22.315
so there's nothing new here.

00:01:22.315 --> 00:01:24.945
The next step is to build our model.

00:01:24.945 --> 00:01:29.660
In this notebook, we're going to use a sequential model with these layers.

00:01:29.660 --> 00:01:31.565
Once we have our model,

00:01:31.565 --> 00:01:33.860
the next step is to train it.

00:01:33.860 --> 00:01:35.870
But before we can train our model,

00:01:35.870 --> 00:01:39.125
we need to set the parameters we're going to use to train it.

00:01:39.125 --> 00:01:41.415
To configure our model for training,

00:01:41.415 --> 00:01:43.335
we use the compile method.

00:01:43.335 --> 00:01:48.665
The main parameters we need to specify in the compile method are the optimizer,

00:01:48.665 --> 00:01:52.765
the loss function, and the metrics we're going to use.

00:01:52.765 --> 00:01:57.260
In this notebook, we will be using the Adam optimizer.

00:01:57.260 --> 00:02:02.365
Adam is an optimization of this stochastic gradient descent algorithm.

00:02:02.365 --> 00:02:05.870
Remember, the optimizer is the algorithm

00:02:05.870 --> 00:02:09.695
that we're going to use to update the weights of our model during training.

00:02:09.695 --> 00:02:14.335
Next, let's take a look at the loss function we're going to use.

00:02:14.335 --> 00:02:19.130
Remember, the loss function is what we use to measure the difference between

00:02:19.130 --> 00:02:24.610
the true labels of the images in our dataset and the predictions made by our model.

00:02:24.610 --> 00:02:30.740
In this notebook, we're going to use the sparse categorical cross entropy loss function.

00:02:30.740 --> 00:02:35.465
We're going to use this loss function because the labels in our dataset are integers,

00:02:35.465 --> 00:02:37.330
as we saw in the previous notebook.

00:02:37.330 --> 00:02:41.305
On the other hand, if the labels of our dataset were one-hot encoded,

00:02:41.305 --> 00:02:43.900
we'll have to use a different loss function.

00:02:43.900 --> 00:02:47.420
Now let's look at the metrics parameter.

00:02:47.420 --> 00:02:53.905
Here we use a list to indicate the metrics we want to evaluate during training.

00:02:53.905 --> 00:02:56.240
In this notebook, we're going to measure

00:02:56.240 --> 00:02:59.930
the accuracy of our model during the training process.

00:02:59.930 --> 00:03:02.255
The accuracy calculates how often

00:03:02.255 --> 00:03:06.890
our model's predictions match the true labels of the images in our dataset.

00:03:06.890 --> 00:03:08.570
Before we train our model,

00:03:08.570 --> 00:03:12.020
let's quickly take a look at the loss and accuracy values that we

00:03:12.020 --> 00:03:16.955
get when we pass a single batch of images to our untrained model.

00:03:16.955 --> 00:03:20.730
To do this, we're going to use the evaluate method.

00:03:20.730 --> 00:03:24.530
The evaluate method compares the predicted output of

00:03:24.530 --> 00:03:30.200
our model on the given image batch with the true labels of our images.

00:03:30.200 --> 00:03:34.310
Then it returns the loss and accuracy values.

00:03:34.310 --> 00:03:43.405
As we can see, we get a loss value of around 2.4 and an accuracy of about 6.3 percent.

00:03:43.405 --> 00:03:45.185
Remember, at this stage,

00:03:45.185 --> 00:03:46.970
our model is untrained,

00:03:46.970 --> 00:03:50.360
so it'd only contains random weights and biases.

00:03:50.360 --> 00:03:54.185
Now let's train our model and see how these numbers improve.

00:03:54.185 --> 00:03:56.795
We expect that after training,

00:03:56.795 --> 00:04:02.435
the last value will be much smaller and that the accuracy will be much higher.

00:04:02.435 --> 00:04:05.990
Ideally, we want our loss value to be close to

00:04:05.990 --> 00:04:10.920
zero and the accuracy will be very close to a 100 percent.

