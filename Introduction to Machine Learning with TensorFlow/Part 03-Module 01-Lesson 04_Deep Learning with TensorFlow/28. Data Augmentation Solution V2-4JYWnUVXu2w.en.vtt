WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.665
So let's take a quick look at my solution for this exercise.

00:00:04.665 --> 00:00:08.820
Remember, you can use all of your own values here just as long as they are actually

00:00:08.820 --> 00:00:12.870
valid values for the different augmentation techniques,

00:00:12.870 --> 00:00:15.915
so you don't need to match to my solution exactly.

00:00:15.915 --> 00:00:19.515
So I'm going to actually reuse a little bit of code from earlier,

00:00:19.515 --> 00:00:22.560
and I'll come up here and just copy

00:00:22.560 --> 00:00:27.330
my image generation and train data generation from earlier.

00:00:27.330 --> 00:00:32.565
So that way, train data gen is already using this, it will be pretty easy.

00:00:32.565 --> 00:00:36.510
So I will want my re-scaling to stay the same,

00:00:36.510 --> 00:00:44.850
and I'm actually going to copy in these to my function,

00:00:44.850 --> 00:00:49.495
so that way I can add them in pretty easily here.

00:00:49.495 --> 00:00:54.515
So I have these in here and I'm just going to set a few different ranges here.

00:00:54.515 --> 00:00:57.440
So rotation again is an angle,

00:00:57.440 --> 00:01:01.565
so I'll set that as a range of 40 degrees.

00:01:01.565 --> 00:01:06.500
Width shift range is the overall percentage of the width to shift left or right,

00:01:06.500 --> 00:01:10.860
so I'm going to put that as 0.2 for about 20 percent,

00:01:10.860 --> 00:01:12.815
and same thing with height shift range.

00:01:12.815 --> 00:01:14.165
Anything more than that,

00:01:14.165 --> 00:01:17.645
you're running the risk that too much of the image is

00:01:17.645 --> 00:01:23.140
off-screen and the neural network won't have any idea what it actually is.

00:01:23.140 --> 00:01:26.765
I think 20 percent is about reasonable for shear,

00:01:26.765 --> 00:01:31.160
which is going to crop off the side or zooming as well.

00:01:31.160 --> 00:01:33.935
Again, similarly to the earlier ones,

00:01:33.935 --> 00:01:36.800
it's just too much of a chance that you're not going to be able to

00:01:36.800 --> 00:01:41.535
see the information that's in the image if you go much more than that.

00:01:41.535 --> 00:01:44.580
Horizontal flip is actually a Boolean value,

00:01:44.580 --> 00:01:46.120
so I'll set that to true,

00:01:46.120 --> 00:01:47.990
and then fill mode,

00:01:47.990 --> 00:01:50.140
I'm going to set that to nearest.

00:01:50.140 --> 00:01:52.820
Again, you can check out the documentation

00:01:52.820 --> 00:01:57.140
for what these are if you're unsure as far as fill mode

00:01:57.140 --> 00:02:00.830
goes and you can also check out

00:02:00.830 --> 00:02:05.725
the documentation for other values you can put in for each of these.

00:02:05.725 --> 00:02:08.540
So if we go ahead and plot this out,

00:02:08.540 --> 00:02:11.480
we can now see multiple different images.

00:02:11.480 --> 00:02:17.425
Again, this is just a single dog image that's been put in to our neural net as an input,

00:02:17.425 --> 00:02:21.605
but it clearly varies quite a bit between the different flipping,

00:02:21.605 --> 00:02:24.605
the zooms, rotations, etc.

00:02:24.605 --> 00:02:30.240
So our network is going to be able to generalize much better than it did before.

00:02:30.640 --> 00:02:34.340
Here's a few further examples as well.

00:02:35.750 --> 00:02:39.320
As you can see here, this would be a potential image

00:02:39.320 --> 00:02:42.860
where I think that there might have been too much augmentation applied.

00:02:42.860 --> 00:02:44.990
I'm guessing this is probably a cat,

00:02:44.990 --> 00:02:46.850
since these other images right here are cats,

00:02:46.850 --> 00:02:48.335
but I can't really tell.

00:02:48.335 --> 00:02:49.550
Maybe this is the ear,

00:02:49.550 --> 00:02:51.590
that looks a little bit more like a cat ear,

00:02:51.590 --> 00:02:53.855
but this is going to be pretty tough to train on.

00:02:53.855 --> 00:02:58.055
Generally, we only applied data augmentation to our train data.

00:02:58.055 --> 00:03:00.260
So for a validation set,

00:03:00.260 --> 00:03:04.555
we only need to perform the normalization on the pixel values.

00:03:04.555 --> 00:03:07.445
However, we're still going to need batch size,

00:03:07.445 --> 00:03:11.870
target size, and the class mode for our flow from directory method.

00:03:11.870 --> 00:03:13.880
For the validation set, remember,

00:03:13.880 --> 00:03:17.455
there is no need really to shuffle the validation set.

00:03:17.455 --> 00:03:20.830
So here, let's go ahead and build a model that can

00:03:20.830 --> 00:03:24.365
classify the images in our dogs verses cats dataset.

00:03:24.365 --> 00:03:27.355
It's going to again be a fairly basic model.

00:03:27.355 --> 00:03:30.760
We're just going to use a lot of fully connected layers,

00:03:30.760 --> 00:03:32.935
you can see the number of nodes here.

00:03:32.935 --> 00:03:38.275
This is maybe a little overly large for our task here,

00:03:38.275 --> 00:03:40.750
but we'll go ahead and do this with

00:03:40.750 --> 00:03:47.660
our input shape and add dropout in-between each fully connected layer.

00:03:47.660 --> 00:03:53.830
At the end here, we'll add a fully connected layer with two outputs for our two classes,

00:03:53.830 --> 00:03:58.070
dog and cat, as well as a softmax activation.

00:03:59.180 --> 00:04:02.825
So since we're on a CPU here,

00:04:02.825 --> 00:04:04.520
again, this is going to take a little bit,

00:04:04.520 --> 00:04:09.210
as you can see, it took about two minutes on that first epoch to train.

00:04:09.210 --> 00:04:14.305
I'm going to train it for about 10 and we're going to see what the accuracy comes out to.

00:04:14.305 --> 00:04:17.840
Now for yourself, as an optional challenge,

00:04:17.840 --> 00:04:22.175
you can attempt to build a network that can classify the dogs verses cats dataset.

00:04:22.175 --> 00:04:24.425
Now, while it's training here,

00:04:24.425 --> 00:04:25.985
we can't see it exactly yet,

00:04:25.985 --> 00:04:29.480
but it's actually only going to get somewhere around 50 percent accuracy,

00:04:29.480 --> 00:04:30.920
which obviously is not very good,

00:04:30.920 --> 00:04:32.915
that's basically just random guessing.

00:04:32.915 --> 00:04:35.165
If it learned that everything was a dog,

00:04:35.165 --> 00:04:36.590
even though it's dogs and cats,

00:04:36.590 --> 00:04:38.935
it could get 50 percent accuracy.

00:04:38.935 --> 00:04:41.540
Now this is a lot more complicated than what we did

00:04:41.540 --> 00:04:44.935
before with MNIST and Fashion-MNIST datasets.

00:04:44.935 --> 00:04:48.230
You probably won't actually get it to work with a fully connected network,

00:04:48.230 --> 00:04:49.895
no matter how deep you make it.

00:04:49.895 --> 00:04:52.970
Since they have three color channels and a resolution

00:04:52.970 --> 00:04:56.240
higher than the 28-by-28 images from before,

00:04:56.240 --> 00:04:58.275
it's probably not going to work.

00:04:58.275 --> 00:05:00.830
So in this next part,

00:05:00.830 --> 00:05:03.305
I'll show you how to use a pre-trained network

00:05:03.305 --> 00:05:05.750
to build a model that can actually solve this problem,

00:05:05.750 --> 00:05:11.220
and even show you what GPU acceleration does to speed up our training.

