WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.250
Another common method to reduce over fitting other than early stopping,

00:00:05.250 --> 00:00:10.410
is called dropout, where we randomly drop neurons in our model during training.

00:00:10.410 --> 00:00:14.415
This forces the network to share information between weights,

00:00:14.415 --> 00:00:17.850
increasing its ability to generalize to new data.

00:00:17.850 --> 00:00:25.230
It's pretty simple to add a dropout layer with tf.keras, it's just tf.keras.

00:00:25.230 --> 00:00:28.575
layers.Dropout with the dropout rate.

00:00:28.575 --> 00:00:32.730
Using this rate, the neural network will randomly

00:00:32.730 --> 00:00:38.910
set that percentage of input units to zero during training.

00:00:38.910 --> 00:00:41.580
This rate argument is a float between zero and

00:00:41.580 --> 00:00:45.710
one that determines the fraction of neurons that will be turned off.

00:00:45.710 --> 00:00:50.760
So a rate of 0.5 will drop 50 percent of the neurons.

00:00:50.920 --> 00:00:54.050
It's important to note that we should never apply

00:00:54.050 --> 00:00:56.690
dropout to the input layer for network or else we

00:00:56.690 --> 00:01:02.365
won't get all the input information necessary to properly train the model.

00:01:02.365 --> 00:01:05.870
Also, remember we only want to use dropout during

00:01:05.870 --> 00:01:09.425
training so that we can prevent over fitting.

00:01:09.425 --> 00:01:12.700
But during inference, we want to use all the neurons in the network,

00:01:12.700 --> 00:01:15.455
tf.keras will take care of this

00:01:15.455 --> 00:01:19.100
automatically so that it uses the dropout layers during training,

00:01:19.100 --> 00:01:22.204
but automatically ignores them during inference.

00:01:22.204 --> 00:01:25.400
Make sure though, if you're using a different library to make

00:01:25.400 --> 00:01:29.345
sure to take out those dropout layers when you perform inference.

00:01:29.345 --> 00:01:34.015
Now we're going to have you guys do an exercise.

00:01:34.015 --> 00:01:36.275
We've used a couple of models so far,

00:01:36.275 --> 00:01:39.500
including just a basic one and one with with early stopping.

00:01:39.500 --> 00:01:46.085
But we want you to go ahead and go through and use tf.keras.layers.Dropout

00:01:46.085 --> 00:01:49.670
and add three layers with a rate of 20 percent to

00:01:49.670 --> 00:01:54.635
our previous model and then train it again on the Fashion-MNIST dataset.

00:01:54.635 --> 00:01:59.100
Let's see if you can get a lower validation loss.

